//
//  main.cpp
//  MetalTest
//
//  Created by lixianglin on 2025/6/26.
//
#import <Foundation/Foundation.h>
#import <Metal/Metal.h>
#import "ConvSimdGroupShader.hpp"

#include <iostream>
#include <Metal/Metal.h>
#include <iostream>
#include <chrono>
#include <vector>
#include <cassert>
#include <cmath>
#include <cstdlib>
using namespace std;
using namespace std::chrono;

#define ftype4 float4
#define ftype float

const char* metalSource = R"metal(
#include <metal_stdlib>
#include <simd/simd.h>
#include <metal_simdgroup>
using namespace metal;

// ========== 类型定义开始 ==========
#if MNN_METAL_FLOAT32
// FP32 模式
typedef float ftype;
typedef float2 ftype2;
typedef float4 ftype4;
typedef float4x4 ftype4x4;
typedef simdgroup_float8x8 simdgroup_T8x8;
#else
// FP16 模式
typedef half ftype;
typedef half2 ftype2;
typedef half4 ftype4;
typedef half4x4 ftype4x4;
typedef simdgroup_half8x8 simdgroup_T8x8;
#endif

#define FLOAT ftype
#define FLOAT2 ftype2
#define FLOAT4 ftype4
#define FLOAT4x4 ftype4x4
// ========== 类型定义结束 ==========

typedef enum : int {
    None  = 0,
    ReLU  = 1,
    ReLU6 = 2,
} conv_activation_type;

inline ftype4 activate(ftype4 value, conv_activation_type type) {
    switch (type) {
        case ReLU:
            return max(value, (ftype4)0);
        case ReLU6:
            return clamp(value, (ftype4)0, (ftype4)6);
        default: // None
            return value;
    }
}
struct conv1x1_constants {
    int input_size;
    int input_slice;
    int output_width;
    int output_height;
    int output_size;
    int output_slice;
    int output_channel;
    int batch;
    int block_size;
    conv_activation_type activation;
    float scale_coef;
};

#if MNN_METAL_FLOAT32_COMPUTER
typedef simdgroup_float8x8 simdgroup_T8x8;
typedef float    FLOAT;
typedef float2   FLOAT2;
typedef float4   FLOAT4;
typedef float4x4 FLOAT4x4;
#else
typedef simdgroup_half8x8 simdgroup_T8x8;
typedef half    FLOAT;
typedef half2   FLOAT2;
typedef half4   FLOAT4;
typedef half4x4 FLOAT4x4;
#endif


#define SIMD_GROUP_WIDTH 32
#define CONV_UNROLL (4)
#define CONV_UNROLL_L (8)

#define INIT_SIMDGROUP_MATRIX(a, b, d) \
    simdgroup_T8x8 sga[a];\
    simdgroup_T8x8 sgb[b];\
    simdgroup_T8x8 sgd[d];\
    for (int i = 0; i < d; i++){\
        sgd[i] = make_filled_simdgroup_matrix<FLOAT, 8>(0.f);\
    }

#define SIMDGROUP_MATRIX_FMA(a, b) \
    for(int j=0; j<b; j++) {\
        for(int i=0; i<a; i++) {\
            simdgroup_multiply_accumulate(sgd[j*a+i], sga[i], sgb[j], sgd[j*a+i]);\
        }\
    }
    
#define SIMDGROUP_MATRIX_STORE(ptr, d) \
    for(int i=0; i<d; i++) {\
        simdgroup_store(sgd[i], ptr + 64*i, 8);\
    }

kernel void conv1x1_gemm_16x16_sg(const device ftype4 *in            [[buffer(0)]],
                            device ftype4 *out                 [[buffer(1)]],
                            constant conv1x1_constants& cst    [[buffer(2)]],
                            const device ftype4 *wt      [[buffer(3)]],
                            const device ftype4 *biasTerms     [[buffer(4)]],
                            uint3 gid                          [[threadgroup_position_in_grid]],
                            uint                  tiitg[[thread_index_in_threadgroup]],
                            uint                  sgitg[[simdgroup_index_in_threadgroup]]) {
    /*
     // Read:
     ftype 0~127   ---> input: [M16, K8]
     ftype 128~255 ---> input: [K8, N16]
     // Write:
     ftype 0~255 ---> input: [N2, M2, M8, N8]
     */
    threadgroup FLOAT4 sdata[64] = {0.f};
    
    INIT_SIMDGROUP_MATRIX(2, 2, 4);
    int rx = gid.x;// M/16
    int uz = gid.y;// N/16
    
    int kl = tiitg / 16;
    int rcl = tiitg % 16;
    
    // boundary limit
    int idx_n4 = (4 * uz + rcl / 4) < cst.output_slice ? (4 * uz + rcl / 4) : (cst.output_slice - 1);
    int idx_m  = (16 * rx + rcl) < cst.input_size * cst.batch ? (16 * rx + rcl) : (cst.input_size * cst.batch - 1);
    
    auto xy_wt = wt +  (idx_n4 * cst.input_slice + 0) * 4 + rcl % 4;// [N/4, K/4, N4, K4]
    auto xy_in0  = in + idx_m + cst.input_size * cst.batch * kl;// [K/4, M, K4]
    auto xy_out = out + (4 * uz + 2 * kl) * cst.output_size * cst.batch + idx_m;// [N/4, M, N4]

    for (int z = kl; z < cst.input_slice; z += 2) {
        sdata[2* rcl + kl] = FLOAT4(*xy_in0);
        xy_in0 += 2 * cst.input_size * cst.batch;

        FLOAT4 w4 = FLOAT4(xy_wt[4 * z]); // [N/4, K/4, N4, K4]
        ((threadgroup FLOAT*)sdata)[128 + (kl * 4 + 0) * 16 + rcl] = w4[0];
        ((threadgroup FLOAT*)sdata)[128 + (kl * 4 + 1) * 16 + rcl] = w4[1];
        ((threadgroup FLOAT*)sdata)[128 + (kl * 4 + 2) * 16 + rcl] = w4[2];
        ((threadgroup FLOAT*)sdata)[128 + (kl * 4 + 3) * 16 + rcl] = w4[3];

        threadgroup_barrier(mem_flags::mem_threadgroup);
        
        simdgroup_load(sga[0], (const threadgroup FLOAT*)sdata, 8);
        simdgroup_load(sga[1], ((const threadgroup FLOAT*)sdata) + 64, 8);
        simdgroup_load(sgb[0], ((const threadgroup FLOAT*)sdata) + 128, 16);
        simdgroup_load(sgb[1], ((const threadgroup FLOAT*)sdata) + 136, 16);
        
        SIMDGROUP_MATRIX_FMA(2, 2);
        threadgroup_barrier(mem_flags::mem_threadgroup);
    }
    
    SIMDGROUP_MATRIX_STORE((threadgroup FLOAT*)sdata, 4);
    
    threadgroup_barrier(mem_flags::mem_threadgroup);
    
    if((16 * rx + rcl) < cst.input_size * cst.batch) {
        if((4 * uz + 2 * kl) < cst.output_slice) {
            xy_out[0] =  activate(ftype4(sdata[(kl * 16 + rcl) * 2 + 0] + FLOAT4(biasTerms[4 * uz + 2 * kl + 0])), cst.activation);
        }
        if((4 * uz + 2 * kl + 1) < cst.output_slice) {
            xy_out[cst.output_size * cst.batch] = activate(ftype4(sdata[(kl * 16 + rcl) * 2 + 1] + FLOAT4(biasTerms[4 * uz + 2 * kl + 1])), cst.activation);
        }
    }
}


kernel void conv1x1_gemm_32x16_sg(const device ftype4 *in            [[buffer(0)]],
                            device ftype4 *out                 [[buffer(1)]],
                            constant conv1x1_constants& cst    [[buffer(2)]],
                            const device ftype4 *wt      [[buffer(3)]],
                            const device ftype4 *biasTerms     [[buffer(4)]],
                            uint3 gid                          [[threadgroup_position_in_grid]],
                            uint                  tiitg[[thread_index_in_threadgroup]],
                            uint                  sgitg[[simdgroup_index_in_threadgroup]]) {
    /*
     // Read:
     ftype 0~255   ---> input: [M32, K8]
     ftype 256~383 ---> input: [K8, N16]
     // Write:
     ftype 0~511 ---> input: [N2, M4, M8, N8]
     */
    threadgroup FLOAT4 sdata[128] = {0.f};
    
    INIT_SIMDGROUP_MATRIX(4, 2, 8);
    
    int rx = gid.x;// M/32
    int uz = gid.y;// N/16
    
    int kl = tiitg % 2;
    int rcl = tiitg / 2;
    
    const int size_m = cst.input_size * cst.batch;
    
    // boundary limit
    int idx_n4 = (4 * uz + rcl / 4) < cst.output_slice ? (4 * uz + rcl / 4) : (cst.output_slice - 1);
    int idx_m0  = (16 * rx + rcl) <  size_m ? (16 * rx + rcl) : (size_m - 1);
    int idx_m1  = (16 * rx + rcl) + size_m / 2 < size_m ? (16 * rx + rcl) + size_m / 2: (size_m - 1);

    auto xy_wt = wt +  (idx_n4 * cst.input_slice + 0) * 4 + rcl % 4;// [N/4, K/4, N4, K4]
    auto xy_in0  = in + idx_m0 + cst.input_size * cst.batch * kl;// [K/4, M2, M/2, K4]
    auto xy_in1  = in + idx_m1 + cst.input_size * cst.batch * kl;// [K/4, M2, M/2, K4]

    auto xy_out0 = out + (4 * uz + 2 * kl) * cst.output_size * cst.batch + idx_m0;// [N/4, M, N4]
    auto xy_out1 = out + (4 * uz + 2 * kl) * cst.output_size * cst.batch + idx_m1;// [N/4, M, N4]
    
    for (int z = kl; z < cst.input_slice; z += 2) {
        sdata[2* rcl + kl] = (FLOAT4)*xy_in0;
        sdata[32 + 2* rcl + kl] = (FLOAT4)*xy_in1;

        FLOAT4 w4 = FLOAT4(xy_wt[4*z]); // [N/4, K/4, N4, K4]
        ((threadgroup FLOAT*)sdata)[256 + (kl * 4 + 0) * 16 + rcl] = w4[0];
        ((threadgroup FLOAT*)sdata)[256 + (kl * 4 + 1) * 16 + rcl] = w4[1];
        ((threadgroup FLOAT*)sdata)[256 + (kl * 4 + 2) * 16 + rcl] = w4[2];
        ((threadgroup FLOAT*)sdata)[256 + (kl * 4 + 3) * 16 + rcl] = w4[3];
        threadgroup_barrier(mem_flags::mem_threadgroup);
        
        simdgroup_load(sga[0], (const threadgroup FLOAT*)sdata, 8);
        simdgroup_load(sga[1], ((const threadgroup FLOAT*)sdata) + 64, 8);
        simdgroup_load(sga[2], ((const threadgroup FLOAT*)sdata) + 128, 8);
        simdgroup_load(sga[3], ((const threadgroup FLOAT*)sdata) + 192, 8);
        
        simdgroup_load(sgb[0], ((const threadgroup FLOAT*)sdata) + 256, 16);
        simdgroup_load(sgb[1], ((const threadgroup FLOAT*)sdata) + 264, 16);
        
        SIMDGROUP_MATRIX_FMA(4, 2);
        threadgroup_barrier(mem_flags::mem_threadgroup);
        
        xy_in0 += 2 * cst.input_size * cst.batch;
        xy_in1 += 2 * cst.input_size * cst.batch;

    }

    SIMDGROUP_MATRIX_STORE((threadgroup FLOAT*)sdata, 8);
    
    threadgroup_barrier(mem_flags::mem_threadgroup);
    
    if((16 * rx + rcl) < size_m) {
        if((4 * uz + 2 * kl) < cst.output_slice) {
            xy_out0[0] =  activate(ftype4(sdata[(kl * 32 + rcl) * 2 + 0] + FLOAT4(biasTerms[4 * uz + 2 * kl + 0])), cst.activation);
        }
        if((4 * uz + 2 * kl + 1) < cst.output_slice) {
            xy_out0[cst.output_size * cst.batch] = activate(ftype4(sdata[(kl * 32 + rcl) * 2 + 1] + FLOAT4(biasTerms[4 * uz + 2 * kl + 1])), cst.activation);
        }
    }
    if((16 * rx + rcl) + size_m / 2 < size_m) {
        if((4 * uz + 2 * kl) < cst.output_slice) {
            xy_out1[0] =  activate(ftype4(sdata[(kl * 32 + 16 + rcl) * 2 + 0] + FLOAT4(biasTerms[4 * uz + 2 * kl + 0])), cst.activation);
        }
        if((4 * uz + 2 * kl + 1) < cst.output_slice) {
            xy_out1[cst.output_size * cst.batch] = activate(ftype4(sdata[(kl * 32 + 16 + rcl) * 2 + 1] + FLOAT4(biasTerms[4 * uz + 2 * kl + 1])), cst.activation);
        }
    }
}

kernel void conv1x1_g1z8_0626(const device ftype4 *in            [[buffer(0)]],
device ftype4 *out                 [[buffer(1)]],
constant conv1x1_constants& cst    [[buffer(2)]],
const device ftype4x4 *wt          [[buffer(3)]],
const device ftype4 *biasTerms     [[buffer(4)]],
uint3 gid                          [[thread_position_in_grid]]) {
if ((int)gid.x * CONV_UNROLL_L >= cst.output_size || (int)gid.y >= cst.output_slice || (int)gid.z >= cst.batch) return;

int rx = gid.x * CONV_UNROLL_L;
int uz = gid.y;
auto xy_wt = wt + uz * cst.input_slice;
auto xy_in0  = in  + (int)gid.z * cst.input_slice  * cst.input_size + rx + 0;

auto xy_out = out + (int)gid.z * cst.output_slice * cst.output_size + uz * cst.output_size + rx;
auto biasValue = ftype4(biasTerms[uz]);
ftype4 result0 = biasValue, result1 = biasValue, result2 = biasValue, result3 = biasValue;
ftype4 result4 = biasValue, result5 = biasValue, result6 = biasValue, result7 = biasValue;

int computeSize = min(cst.output_size - rx, CONV_UNROLL_L);
for (auto z = 0; z < cst.input_slice; z++) {
        auto in40 = xy_in0[0];
        auto in41 = xy_in0[1];
        auto in42 = xy_in0[2];
        auto in43 = xy_in0[3];
        auto in44 = xy_in0[4];
        auto in45 = xy_in0[5];
        auto in46 = xy_in0[6];
        auto in47 = xy_in0[7];

        auto w = xy_wt[z];

        result0 += ftype4(in40 * w);
        result1 += ftype4(in41 * w);
        result2 += ftype4(in42 * w);
        result3 += ftype4(in43 * w);
        result4 += ftype4(in44 * w);
        result5 += ftype4(in45 * w);
        result6 += ftype4(in46 * w);
        result7 += ftype4(in47 * w);
        xy_in0 += cst.input_size;
}

/* true                               */ *xy_out = activate(ftype4(result0), cst.activation);
if (computeSize > 1) {xy_out[1] = activate(ftype4(result1), cst.activation); }
if (computeSize > 2) {xy_out[2] = activate(ftype4(result2), cst.activation); }
if (computeSize > 3) {xy_out[3] = activate(ftype4(result3), cst.activation); }
if (computeSize > 4) {xy_out[4] = activate(ftype4(result4), cst.activation); }
if (computeSize > 5) {xy_out[5] = activate(ftype4(result5), cst.activation); }
if (computeSize > 6) {xy_out[6] = activate(ftype4(result6), cst.activation); }
if (computeSize > 7) {xy_out[7] = activate(ftype4(result7), cst.activation); }

}

)metal";


template <class T>
bool CreateInputTemplateData(T *in_data, const uint32_t data_len) {
    if (nullptr == in_data) {
        fprintf(stderr, "input data is null!\n");
        return false;
    }
    if (sizeof(float) == sizeof(T)) {
        for (int i = 0; i < data_len; i++) {
            in_data[i] = ((float)(i % 255)) / 255.0;
        }
    } else if (sizeof(uint8_t) == sizeof(T)) {
        for (int i = 0; i < data_len; i++) {
            in_data[i] = i % 255;
        }
    } else {
        return false;
    }
    return true;
}

// 辅助函数：创建FP16缓冲区
id<MTLBuffer> createFP16Buffer(id<MTLDevice> device, size_t elements, const void* data = nullptr) {
    id<MTLBuffer> buffer = [device newBufferWithLength:elements * sizeof(__fp16)
                                             options:MTLResourceStorageModeShared];
    if (data) {
        // 将float数据转换为fp16
        const float* floatData = static_cast<const float*>(data);
        __fp16* fp16Data = static_cast<__fp16*>(buffer.contents);
        for (size_t i = 0; i < elements; ++i) {
            fp16Data[i] = floatData[i];
        }
    }
    return buffer;
}

// 结构体匹配Metal着色器中的常量
struct Conv1x1Constants {
    int input_size;
    int input_slice;
    int output_width;
    int output_height;
    int output_size;
    int output_slice;
    int output_channel;
    int batch;
    int block_size;
    int activation;
    float scale_coef;
};



// 测试conv1x1_g1z8_0626核函数（FP16版本）
void testG1Z8KernelFP16(id<MTLDevice> device, id<MTLCommandQueue> queue,
                       int batch, int height, int width, int inChannels, int outChannels) {
    NSError* error = nil;
    
    // 使用FP16版本的核函数
    NSString* kernelSource = [NSString stringWithUTF8String:metalSource];
    MTLCompileOptions* options = [[MTLCompileOptions alloc] init];
    options.preprocessorMacros = @{
        @"ftype4": @"float4",
        @"ftype": @"float"
    };
    options.fastMathEnabled = YES;
    options.languageVersion = MTLLanguageVersion2_3; // 需要 Metal 2.3 或更高版本

    // 设置预处理器宏
    NSDictionary* macros = @{
        @"MNN_METAL_FLOAT32": @"0",  // 0表示使用FP16(half)
        @"W_QUANT_4": @"1"           // 如果需要4位量化
    };
    options.preprocessorMacros = macros;
    
    id<MTLLibrary> library = [device newLibraryWithSource:kernelSource
                                                options:options
                                                  error:&error];
    if (!library) {
        NSLog(@"创建库失败: %@", error);
        return;
    }
    
    id<MTLFunction> kernelFunction = [library newFunctionWithName:@"conv1x1_g1z8_0626"];
    id<MTLComputePipelineState> pipeline = [device newComputePipelineStateWithFunction:kernelFunction error:&error];
    if (!pipeline) {
        NSLog(@"创建管线失败: %@", error);
        return;
    }
    
    // 准备FP16测试数据
    const int inputElements = batch * height * width * inChannels;
    const int outputElements = batch * height * width * outChannels;
    const int weightElements = outChannels * inChannels;
    
    std::vector<float> inputData(inputElements);
    std::vector<float> weightData(weightElements);
    std::vector<float> biasData(outChannels);
    
    // 使用随机数据填充
    CreateInputTemplateData<float>(inputData.data(), inputElements);
    CreateInputTemplateData<float>(weightData.data(), weightElements);
    CreateInputTemplateData<float>(biasData.data(), outChannels);
    
    // 创建FP16缓冲区
    id<MTLBuffer> inputBuffer = createFP16Buffer(device, inputElements, inputData.data());
    id<MTLBuffer> outputBuffer = createFP16Buffer(device, outputElements);
    id<MTLBuffer> weightBuffer = createFP16Buffer(device, weightElements, weightData.data());
    id<MTLBuffer> biasBuffer = createFP16Buffer(device, outChannels, biasData.data());
    
    // 设置常量参数
    Conv1x1Constants constants{
        .input_size = height * width,
        .input_slice = (inChannels + 3) / 4,
        .output_width = width,
        .output_height = height,
        .output_size = height * width,
        .output_slice = (outChannels + 3) / 4,
        .output_channel = outChannels,
        .batch = batch,
        .block_size = 1,
        .activation = 0, // None
        .scale_coef = 1.0f
    };
    id<MTLBuffer> constantsBuffer = [device newBufferWithBytes:&constants
                                                      length:sizeof(Conv1x1Constants)
                                                     options:MTLResourceStorageModeShared];
    
    // 创建命令缓冲区
    id<MTLCommandBuffer> commandBuffer = [queue commandBuffer];
    id<MTLComputeCommandEncoder> encoder = [commandBuffer computeCommandEncoder];
    
    auto start = std::chrono::high_resolution_clock::now();
    // 配置管线
    [encoder setComputePipelineState:pipeline];
    [encoder setBuffer:inputBuffer offset:0 atIndex:0];
    [encoder setBuffer:outputBuffer offset:0 atIndex:1];
    [encoder setBuffer:constantsBuffer offset:0 atIndex:2];
    [encoder setBuffer:weightBuffer offset:0 atIndex:3];
    [encoder setBuffer:biasBuffer offset:0 atIndex:4];
    
    // 配置线程组
    MTLSize threadsPerThreadgroup = MTLSizeMake(pipeline.threadExecutionWidth, 1, 1);
    MTLSize threadgroups = MTLSizeMake((width * height + 7) / 8,
                                      (outChannels + 3) / 4,
                                      batch);
    
    // 执行核函数并测量时间
    //auto start = std::chrono::high_resolution_clock::now();
    
    [encoder dispatchThreadgroups:threadgroups
           threadsPerThreadgroup:threadsPerThreadgroup];
    [encoder endEncoding];
    [commandBuffer commit];
    [commandBuffer waitUntilCompleted];
    
    auto end = std::chrono::high_resolution_clock::now();
    auto duration = std::chrono::duration_cast<std::chrono::microseconds>(end - start).count();
    
    std::cout << "    conv1x1_g1z8_0626 (FP16) 执行时间: " << duration << " μs" << std::endl;
}

// 测试conv1x1_gemm_32x16_sg核函数（FP16版本）
void testGemm32x16KernelFP16(id<MTLDevice> device, id<MTLCommandQueue> queue,
                            int batch, int height, int width, int inChannels, int outChannels) {
    NSError* error = nil;
    
    NSString* kernelSource = [NSString stringWithUTF8String:metalSource];
    MTLCompileOptions* options = [[MTLCompileOptions alloc] init];
//    options.fastMathEnabled = YES;
//    options.languageVersion = MTLLanguageVersion2_1;
//    
//    id<MTLLibrary> library = [device newLibraryWithSource:kernelSource
//                                                options:options
//                                                  error:&error];
    
    options.preprocessorMacros = @{
        @"ftype4": @"float4",
        @"ftype": @"float"
    };
    options.fastMathEnabled = YES;
    options.languageVersion = MTLLanguageVersion2_3; // 需要 Metal 2.3 或更高版本

    // 设置预处理器宏
    NSDictionary* macros = @{
        @"MNN_METAL_FLOAT32": @"0",  // 0表示使用FP16(half)
        @"W_QUANT_4": @"1"           // 如果需要4位量化
    };
    options.preprocessorMacros = macros;
    
    id<MTLLibrary> library = [device newLibraryWithSource:kernelSource
                                                options:options
                                                  error:&error];
    if (!library) {
        NSLog(@"创建库失败2: %@", error);
        return;
    }
    id<MTLFunction> kernelFunction = [library newFunctionWithName:@"conv1x1_gemm_32x16_sg"];
    id<MTLComputePipelineState> pipeline = [device newComputePipelineStateWithFunction:kernelFunction error:&error];
    
    // 准备FP16测试数据（与testG1Z8KernelFP16相同）
    const int inputElements = batch * height * width * inChannels;
    const int outputElements = batch * height * width * outChannels;
    const int weightElements = outChannels * inChannels;
    
    std::vector<float> inputData(inputElements);
    std::vector<float> weightData(weightElements);
    std::vector<float> biasData(outChannels);
    
    // 使用随机数据填充
    CreateInputTemplateData<float>(inputData.data(), inputElements);
    CreateInputTemplateData<float>(weightData.data(), weightElements);
    CreateInputTemplateData<float>(biasData.data(), outChannels);
    
    id<MTLBuffer> inputBuffer = createFP16Buffer(device, inputElements, inputData.data());
    id<MTLBuffer> outputBuffer = createFP16Buffer(device, outputElements);
    id<MTLBuffer> weightBuffer = createFP16Buffer(device, weightElements, weightData.data());
    id<MTLBuffer> biasBuffer = createFP16Buffer(device, outChannels, biasData.data());
    
    Conv1x1Constants constants{
        .input_size = height * width,
        .input_slice = (inChannels + 3) / 4,
        .output_width = width,
        .output_height = height,
        .output_size = height * width,
        .output_slice = (outChannels + 3) / 4,
        .output_channel = outChannels,
        .batch = batch,
        .block_size = 1,
        .activation = 0,
        .scale_coef = 1.0f
    };
    id<MTLBuffer> constantsBuffer = [device newBufferWithBytes:&constants
                                                     length:sizeof(Conv1x1Constants)
                                                    options:MTLResourceStorageModeShared];
    
    id<MTLCommandBuffer> commandBuffer = [queue commandBuffer];
    id<MTLComputeCommandEncoder> encoder = [commandBuffer computeCommandEncoder];
    
    auto start = std::chrono::high_resolution_clock::now();
    [encoder setComputePipelineState:pipeline];
    [encoder setBuffer:inputBuffer offset:0 atIndex:0];
    [encoder setBuffer:outputBuffer offset:0 atIndex:1];
    [encoder setBuffer:constantsBuffer offset:0 atIndex:2];
    [encoder setBuffer:weightBuffer offset:0 atIndex:3];
    [encoder setBuffer:biasBuffer offset:0 atIndex:4];
    
    // GEMM核函数的线程配置不同
    MTLSize threadsPerThreadgroup = MTLSizeMake(32, 1, 1);
    MTLSize threadgroups = MTLSizeMake((batch * height * width + 31) / 32,
                                      (outChannels + 15) / 16,
                                      1);
    
    [encoder dispatchThreadgroups:threadgroups
           threadsPerThreadgroup:threadsPerThreadgroup];
    [encoder endEncoding];
    [commandBuffer commit];
    [commandBuffer waitUntilCompleted];
    
    auto end = std::chrono::high_resolution_clock::now();
    auto duration = std::chrono::duration_cast<std::chrono::microseconds>(end - start).count();
    
    std::cout << "    conv1x1_gemm_32x16_sg (FP16) 执行时间: " << duration << " μs" << std::endl;
}



void Run()
{
//    NSArray<id<MTLDevice>> *devices = MTLCopyAllDevices();
//    std::cout << "Metal 可用设备数量: " << devices.count << std::endl;
//    for (id<MTLDevice> dev in devices) {
//        std::cout << "设备名: " << dev.name.UTF8String << std::endl;
//    }
//    // 创建 Metal 设备
//    id<MTLDevice> device = MTLCreateSystemDefaultDevice();
//    if (!device) {
//        cerr << "Metal is not supported on this device" << endl;
//        return ;
//    }
    id<MTLDevice> device = MTLCreateSystemDefaultDevice();
    if (!device) {
        std::cerr << "Metal不支持此设备" << std::endl;
        return ;
    }
    
    // 检查FP16支持
//    if (!device.supportsFeatureSet(MTLFeatureSet_macOS_GPUFamily1_v3)) {
//        std::cerr << "此设备不支持FP16计算" << std::endl;
//        return ;
//    }
    
    // 改为（macOS 方式）：
    if (![device supportsFamily:MTLGPUFamilyMetal3]) {
        std::cerr << "此设备不支持所需的Metal功能集" << std::endl;
        return ;
    }
    
    id<MTLCommandQueue> queue = [device newCommandQueue];
    
    // 测试参数：1x1024x1024x22输入，16输出通道
    const int batch = 1;
    const int height = 1024;
    const int width = 1024;
    const int inChannels = 22;
    const int outChannels = 16;
    
    std::cout << "FP16精度测试 - 输入: " << batch << "x" << height << "x" << width << "x" << inChannels
              << ", 输出通道: " << outChannels << std::endl;
    
    // 预热运行
    testG1Z8KernelFP16(device, queue, batch, height, width, inChannels, outChannels);
    testGemm32x16KernelFP16(device, queue, batch, height, width, inChannels, outChannels);
    
    // 正式测试运行
    const int numRuns = 10;
    std::vector<long> g1z8Times(numRuns);
    std::vector<long> gemmTimes(numRuns);
    
    for (int i = 0; i < numRuns; ++i) {
        auto start = std::chrono::high_resolution_clock::now();
        testG1Z8KernelFP16(device, queue, batch, height, width, inChannels, outChannels);
        auto end = std::chrono::high_resolution_clock::now();
        g1z8Times[i] = std::chrono::duration_cast<std::chrono::microseconds>(end - start).count();
        
        start = std::chrono::high_resolution_clock::now();
        testGemm32x16KernelFP16(device, queue, batch, height, width, inChannels, outChannels);
        end = std::chrono::high_resolution_clock::now();
        gemmTimes[i] = std::chrono::duration_cast<std::chrono::microseconds>(end - start).count();
    }
    
    // 计算统计信息
    long g1z8Sum = 0, gemmSum = 0;
    long g1z8Min = LONG_MAX, gemmMin = LONG_MAX;
    long g1z8Max = 0, gemmMax = 0;
    
    for (int i = 0; i < numRuns; ++i) {
        g1z8Sum += g1z8Times[i];
        gemmSum += gemmTimes[i];
        
        g1z8Min = std::min(g1z8Min, g1z8Times[i]);
        gemmMin = std::min(gemmMin, gemmTimes[i]);
        
        g1z8Max = std::max(g1z8Max, g1z8Times[i]);
        gemmMax = std::max(gemmMax, gemmTimes[i]);
    }
    
    double g1z8Avg = static_cast<double>(g1z8Sum) / numRuns;
    double gemmAvg = static_cast<double>(gemmSum) / numRuns;
    
    // 打印结果
    std::cout << "\nFP16精度性能测试结果 (" << numRuns << "次运行):" << std::endl;
    std::cout << "conv1x1_g1z8_0626 - 平均: " << g1z8Avg << " μs, 最小: " << g1z8Min
              << " μs, 最大: " << g1z8Max << " μs" << std::endl;
    std::cout << "conv1x1_gemm_32x16_sg - 平均: " << gemmAvg << " μs, 最小: " << gemmMin
              << " μs, 最大: " << gemmMax << " μs" << std::endl;
    
    // 计算加速比
    if (g1z8Avg > 0 && gemmAvg > 0) {
        double speedup = g1z8Avg / gemmAvg;
        std::cout << "\n加速比 (g1z8/gemm): " << speedup << "x" << std::endl;
        
        if (speedup > 1.0) {
            std::cout << "conv1x1_gemm_32x16_sg (FP16) 快 " << speedup << " 倍" << std::endl;
        } else {
            std::cout << "conv1x1_g1z8_0626 (FP16) 快 " << (1.0 / speedup) << " 倍" << std::endl;
        }
    }
    
}

int main0627(int argc, const char * argv[]) {
    @autoreleasepool {
        // insert code here...
        std::cout << "Hello, World!\n";
        
        Run();
    }
    return 0;
}
